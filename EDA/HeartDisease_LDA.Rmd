---
title: "Heart Disease Prediction LDA "
author: "Thomas Rowley"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
      number_sections: false
      extra_dependencies: "bbm"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, out.width="80%",
                      fig.align="center", fig.pos = 'H', extra.out="")
library(ggthemes)
library(ggplot2)
library(ggExtra)
library(gridExtra)
library(GGally)
library(kableExtra)
require(klaR)
require(ggord)
library(cvms)
library(caret)
require(covequal)
library(pander)

theme_set(theme_minimal())
theme_set(theme_get() + theme(text = element_text(family = 'serif')))
set.seed(1234567890)
```

```{r}
hd <- read.csv("Heart_Disease_Prediction.csv")
hd <- as.data.frame(hd[,-1])
```

A pairs plot can be generated to show visually the difference in distribution between heart disease presence and absence, found in Figure \@ref(fig:pairs).

```{r pairs, fig.cap="Pairs plot for the continuous numeric variables.", message=FALSE}
ggpairs(hd, aes(colour=Heart.Disease, alpha=.5), columns=c("Age", "BP", 
  "Cholesterol", "Max.HR")) + 
  theme_pander(base_size = 8)
```

Histograms of Age and maximum heart rate appear to differ significantly depending on heart disease presence or absence, indicating these variables may be significant in prediction. Blood pressure and cholesterol appears to be slightly higher for those with heart disease than those without.

A linear discriminant analysis is performed, randomly sampling data with replacement to the training and test sets with 80% and 20% probabilities respectively. The analysis is performed accounting for all numerical variables in the dataset, i.e. age, blood pressure, cholesterol, maximum heart rate. It also takes into account chest pain type and sex of the patient.

```{r LDA}
ind <- sample(c("Tr", "Te"),
              nrow(hd),
              replace=TRUE,
              prob=c(0.8,0.2))

Train <- hd[ind=="Tr",]
Test <- hd[ind=="Te",]

LDA <- lda(Heart.Disease ~ Age + BP + Cholesterol + Max.HR +
             as.factor(Chest.pain.type) + as.factor(Sex), data=Train)
```

The results of the analysis can be seen in Figure \@ref(fig:LDAhist). In this histogram, overlap implies that the person was assigned their class of heart disease wrongly, i.e. a false positive or false negative.

```{r LDAhist, fig.cap="Histogram of assigned class of heart disease."}
Pred <- predict(LDA)
ldahist(data=Pred$x, g=Train$Heart.Disease)
```

There is considerable overlap between the interval $[-1, 1]$, indicating some misclassification of data. Another look at the class assignments is given in Figure \@ref(fig:assignHist), where smooth density curves are given. Overlap in the histogram again represents misclassification of patients.

```{r assignHist, fig.cap="Density plot of assigned class of heart disease."}
df <- data.frame(Pred$x, Pred$class)
ggplot(df, aes(x=Pred$x, fill=Pred$class))+geom_density(alpha=0.5) +
  labs(fill="Class") + ylab("Density") + xlab("x")
```

From Figure \@ref(fig:assignHist), it is obvious there is a small amount of error in the prediction model. Confusion matrices can be produced to quantify this error. The realistic confusion matrix is found in Table \@ref(tab:realisticConfMat).

<!--- Optimistic confusion matrix, should not be reported.
```{r}
OptimisticPredicted <- predict(LDA, Train)$class
(OCM <- table(OptimisticPredicted, Actual=Train$Heart.Disease))
sum(diag(OCM))/sum(OCM) # Overall accuracy
```
-->

```{r realisticConfMat}
RealisticPredicted <- predict(LDA, Test)$class
RCM <- table(RealisticPredicted, Actual=Test$Heart.Disease)
LDACMstats <- confusionMatrix(RCM, positive = "Presence")
knitr::kable(LDACMstats$table, booktabs=TRUE, 
             caption="Confusion matrix for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The linear discriminant model has wrongly assigned 7 patients without heart disease as having heart disease, and 3 patients with heart disease as not having heart disease, errors of $22.58\%$ and $7.69\%$ respectively.

More detailed statistics concerning the confusion matrix can be given by the `confusionMatrix()` function, presented in Table \@ref(tab:LDAConfMatStats) and Table \@ref(tab:LDAOverallStats).

```{r LDAOverallStats}
knitr::kable(scales::percent(LDACMstats$overall, accuracy=0.01), booktabs=TRUE, 
             caption="Overall statistics for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The linear discriminant model has an overall accuracy of $85.71\% \pm 10.42\%$. The accuracy of the model is tested against the No Information Rate, which is the largest proportion of any class in the model, i.e. $\text{P}(Absence)$, at the $\alpha=5\%$ significance level. The p-value for the significance of this model with respect to the No Information Rate of $55.71\%$ is $p\approx0.00\%$ at the $\alpha = 5\%$ significance level, meaning the linear discriminant analysis is highly significant in its prediction power for heart disease over the no-information model.

```{r LDAConfMatStats}
knitr::kable(scales::percent(LDACMstats$byClass, accuracy=0.01), booktabs=TRUE, 
             caption="Prediction statistics for predicting heart disease using linear discriminant analysis.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The model has sensitivity of $77.42\%$, specificity of $92.31\%$, and a balanced accuracy of $84.86\%$. This means it can predict someone with heart disease as having it with $77.42\%$ certainty, and someone without heart disease as not having it with $92.31\%$ certainty.

An assumption for performing linear discriminant analysis is that covariance matrices between classes are similar. The test for covariance equality is found in Table \@ref(tab:covEquality).

```{r covEquality}
HDabsence <- as.matrix(subset(Train, Train$Heart.Disease=="Absence")[,c(-14)], ncol=5)
HDpresence <- as.matrix(subset(Train, Train$Heart.Disease=="Presence")[,c(-14)], ncol=5)
pander(as.data.frame(test_covequal(HDabsence, HDpresence)),
       caption="(\\#tab:covEquality) Test results for comparing covariance matrices for 
       absence and presence of heart disease for equality.")
```

The covariance equality test is performed at the $\alpha = 0.05$ significance level, with null hypothesis that covariance matrices are equal. The associated p-value for this test is $p=0.1778$. Since $p > \alpha$, there is insufficient evidence to reject that covariance matrices are equal. The assumption required for linear discriminant analysis thus holds.

A partition matrix is generated considering all numerical variables as before. Red points represent that the observation has been misclassified, and black points represent correctly classified observations. The partition plot can be found in Figure \@ref(fig:HDPredictpartimat).

```{r HDPredictpartimat, fig.cap="Partition plot of observations vs numerical variables, with each observation coloured by the quality of their classification."}
partimat(as.factor(Heart.Disease) ~ Age + BP + Cholesterol + Max.HR,
         data=Train, method="lda")
```

The approximate error rate for classification is highest at $43\%$ between blood pressure and age, and lowest at $29.5\%$ between maximum heart rate and cholesterol. This gives a range of classification error between $29.5%-43%$ between the numerical variables of the linear discriminant model.

Instead of linear discriminant analysis, we can do a Bayesian classification instead. The Bayesian method will be tested on the same training data as the LDA was, and tested on the same test data.

```{r}
library(naivebayes)

#necessary transformation for factors
Train$Chest.pain.type <- as.factor(Train$Chest.pain.type)
Train$Sex <- as.factor(Train$Sex)

Bayes <- naivebayes::gaussian_naive_bayes(x=data.matrix(Train[,c(1,2,3,4,5,8)]), y=Train$Heart.Disease, data=Train)
```

This gives the prior probabilities of an observation belonging to either distribution as $\text{P}(\text{Absence}) = \frac{111}{200} = 0.555$ and $\text{P}(\text{Presence}) = \frac{89}{200} = 0.445$, as obtained from the relative proportion of absence and presence in the original observations.

Using these prior probabilities a Bayesian classification method can be applied to test data for variables Age, Sex, chest pain type, blood pressure, cholesterol level, and maximum heart rate, and its performance compared to the LDA. Tables of results can be found in Table \@ref(tab:bayesConfMat), Table \@ref(tab:bayesOverallStats), and Table \@ref(tab:bayesConfMatStats).

```{r bayesConfMat}
BayesPred <- predict(Bayes, newdata=data.matrix(Test[,c(1,2,3,4,5,8)]), type="class")
RCMBayes <- table(BayesPred, Actual=Test$Heart.Disease)
BayesCMstats <- confusionMatrix(RCMBayes, positive="Presence")
knitr::kable(BayesCMstats$table, booktabs=TRUE, 
             caption="Confusion matrix for predicting heart disease using the Bayesian classification method.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification method correctly classified all patients without heart disease, but misclassified 21 of 31 patients with heart disease as not having it, an error of $67.74\%$.

```{r bayesOverallStats}
knitr::kable(scales::percent(BayesCMstats$overall, accuracy=0.01), booktabs=TRUE, 
             caption="Overall statistics for predicting heart disease using the Bayesian classification method.") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification has an overall accuracy of $70.00\% \pm 12.13\%$. The accuracy of this model is tested against the null hypothesis that the model is no better than the largest proportion of classes, the No Information Rate. The `confusionMatrix` function tests this at the $\alpha=5\%$ level, and the corresponding p-value is $1.02\%$. Since $p < \alpha$, there is sufficient evidence to suggest the model is better at predicting levels of heart disease than the no information model. This means the Bayesian classification method is significant in its prediction power for heart disease.


```{r bayesConfMatStats}
knitr::kable(scales::percent(BayesCMstats$byClass, accuracy=0.01), booktabs=TRUE, 
             caption="Prediction statistics for predicting heart disease using the Bayesian classification method") %>%
  kable_styling(latex_options = c("HOLD_position"))
```

The Bayesian classification has sensitivity of $32.26\%$, specificity of $100.00\%$, and a balanced accuracy of $66.13\%$ for predicting the presence of heart disease. This means that the model is highly inaccurate for telling people with heart disease that they have it, but perfect at telling someone without heart disease that they don't have it.


